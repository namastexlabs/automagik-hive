# ğŸ§âœ¨ WISH: Knowledge System INCREMENTAL Cleanup & Refactoring

**Wish ID**: knowledge-system-cleanup-2025-01-28  
**Status**: ğŸ“‹ Planning Phase  
**Complexity**: 9/10 - Major architectural refactoring WITH ZERO DOWNTIME  
**Approach**: ğŸ”ª **SURGICAL EXTRACTION with SAFETY-FIRST PRINCIPLES**  
**Philosophy**: "If it works, don't rewrite it - just organize it better" with incremental safety-first migration  
**Execution Strategy**: Progressive refinement through surgical code extraction while preserving ALL working functionality  

## ğŸš¨ REVISED APPROACH - ULTRA-DEEP ANALYSIS COMPLETE

### Dead Code Discovery (300+ Lines Found!)
After comprehensive analysis, we found **~300 lines of dead code**:
- **ENTIRE FILE DEAD**: `metadata_csv_reader.py` (229 lines) - only used in tests
- **Dead Functions**: `test_config_filter()` (36 lines), `main()` in loader (35 lines)
- **Unused Imports**: PgVector, OpenAIEmbedder in csv_hot_reload.py
- **Commented Code**: Multiple locations with stale comments

### Organizational Chaos
- **Everything in one folder**: 7 Python files all in `/lib/knowledge/`
- **No separation of concerns**: Database, business logic, config all mixed
- **God class**: SmartIncrementalLoader (751 lines) doing everything

## ğŸ“‹ Executive Summary

**SURGICAL** refactoring of the `/lib/knowledge/` system to achieve:
- **ZERO dead code** - 300+ lines removed (~300 found in deep analysis)
- **ZERO naming violations** - Full CLAUDE.md compliance  
- **ZERO architectural violations** - Clean subfolder organization
- **100% KISS compliance** - Simplified with proper structure
- **Full test coverage** - All tests passing as final validation

## ğŸ“‚ NEW FOLDER ORGANIZATION (Clean Architecture)

```
lib/knowledge/
â”œâ”€â”€ core/                    # Core interfaces and base classes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ base.py             # KnowledgeBase abstract class (extracted)
â”‚
â”œâ”€â”€ repositories/            # Database layer (ALL SQL goes here)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ knowledge_repository.py  # All database operations
â”‚
â”œâ”€â”€ services/               # Business logic (algorithms stay intact)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ incremental_loader.py   # Refactored from SmartIncrementalLoader
â”‚   â”œâ”€â”€ hash_manager.py         # Hash calculation logic
â”‚   â””â”€â”€ change_analyzer.py      # Change detection logic
â”‚
â”œâ”€â”€ datasources/            # Data access layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ csv_datasource.py      # CSV file operations
â”‚   â””â”€â”€ csv_hot_reload.py      # File watching (moved)
â”‚
â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.yaml            # All settings (moved)
â”‚   â””â”€â”€ manager.py            # ConfigManager (consolidated)
â”‚
â”œâ”€â”€ filters/                # Business filters
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ business_unit_filter.py  # Renamed from config_aware_filter
â”‚
â”œâ”€â”€ factories/              # Object creation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ knowledge_factory.py    # Factory pattern (moved)
â”‚
â””â”€â”€ data/                   # Data files
    â””â”€â”€ knowledge_rag.csv      # CSV data (moved)
```

## ğŸ¯ Success Criteria

### Quantitative Metrics (MUST ACHIEVE ALL)
- [ ] **Dead code**: ZERO (removing 300+ lines identified)
- [ ] **File size**: ALL files < 350 lines (split 751-line god class)
- [ ] **Naming violations**: ZERO (all methods/variables follow standards)
- [ ] **Architectural violations**: ZERO (proper subfolder organization)
- [ ] **KISS compliance**: 100% (no over-engineering)
- [ ] **Test coverage**: ALL existing tests still pass
- [ ] **SQL separation**: 100% of queries in repositories/

### Qualitative Goals
- [ ] SOLID principles compliance
- [ ] Repository pattern implementation
- [ ] Dependency injection throughout
- [ ] Clear separation of concerns
- [ ] Comprehensive documentation
- [ ] Type hints on all methods

## ğŸ”„ ALTERNATIVE SURGICAL APPROACH INTEGRATION

### Core Surgical Principles (Refined Strategy)
1. **NO ALGORITHM CHANGES** - The logic that works stays EXACTLY the same
2. **EXTRACT, DON'T REWRITE** - Move code blocks, don't recreate them  
3. **SQL SEPARATION ONLY** - Pull database queries into repository layer
4. **PRESERVE FUNCTIONALITY** - Every method keeps its current behavior
5. **FIX VIOLATIONS** - Only change names and structure, not logic

### Surgical Phase Integration
- **Phase 1**: Dead code removal (quick & safe) 
- **Phase 2**: Extract SQL to repository (cut & paste)
- **Phase 3**: Split god class (extract logical units)
- **Phase 4**: Fix naming & organization
- **Phase 5**: Validation with behavioral equivalence

This surgical approach is integrated into the comprehensive safety-first migration strategy below.

## ğŸ”ª Phase 1: Dead Code Elimination (IMMEDIATE - 300+ Lines)

### T0.0: Create Comprehensive Test Baseline
**Owner**: hive-testing-maker  
**Dependencies**: None  
**Parallel**: No - MUST complete first  
**Priority**: CRITICAL - Ensures we don't break working functionality

#### Tasks:
1. **Document current behavior**:
   - Create integration tests that capture CURRENT working behavior
   - Test CSV hot reload functionality
   - Test incremental loading with real data
   - Test business unit filtering
   - Test database sync operations
   
2. **Create performance baseline**:
   - Benchmark current load times
   - Document memory usage patterns
   - Record query performance metrics

3. **Backup strategy**:
   ```bash
   # Create snapshot branch
   git checkout -b knowledge-system-backup-2025-01-28
   
   # Document current working state
   make dev &
   curl http://localhost:8886/api/v1/health
   # Run knowledge system tests
   ```

**Validation**: ALL existing functionality must pass before proceeding

### T0.1: Create Feature Flags for Migration
**Owner**: hive-dev-coder  
**Dependencies**: T0.0  
**Parallel**: No  

#### Tasks:
```yaml
# lib/knowledge/config.yaml - Add migration flags
migration_flags:
  use_new_repository: false  # Switch to new repository pattern
  use_new_datasource: false  # Switch to new CSV datasource
  use_new_loader: false      # Switch to refactored loader
  enable_parallel_run: true  # Run old and new in parallel for validation
```

## ğŸ—ï¸ Phase 1: Foundation & SAFE Dead Code Elimination

### T1.0: Initial Cleanup & Dead Code Removal (REVISED - SAFE)
**Owner**: hive-dev-fixer  
**Dependencies**: T0.0, T0.1  
**Parallel**: No - Must validate each removal  

#### Tasks:
1. **Remove ONLY confirmed dead code** (WITH VALIDATION):
   - Delete unused `MetadataCSVReader` class from `metadata_csv_reader.py`
   - Remove `main()` testing function from `smart_incremental_loader.py` (lines 716-750)
   - Remove `test_config_filter()` from `config_aware_filter.py` (lines 186-221)
   - Clean unused imports: `PgVector`, `OpenAIEmbedder` from `csv_hot_reload.py`
   - Remove commented code blocks throughout

2. **Move magic numbers to YAML configuration**:
   ```yaml
   # Update lib/knowledge/config.yaml
   knowledge_settings:
     processing:
       default_batch_size: 10
       max_preview_length: 100
       max_file_size_lines: 350  # CLAUDE.md compliance
     
     caching:
       default_cache_ttl: 300
     
     embedding:
       default_model: "text-embedding-3-small"
   ```
   
   Then update code to read from config:
   ```python
   # In code files, replace hardcoded values with:
   config = load_knowledge_config()
   batch_size = config.get('knowledge_settings.processing.default_batch_size', 10)
   # etc.
   ```

3. **Fix immediate security issues**:
   - Parameterize all SQL queries properly
   - Validate file paths against directory traversal
   - Sanitize CSV input data

**Validation**: `uv run ruff check lib/knowledge/ --fix && uv run mypy lib/knowledge/`

### T1.1: Create Test Infrastructure
**Owner**: hive-testing-maker  
**Dependencies**: T1.0  
**Parallel**: Can start after T1.0  

#### Tasks:
1. Create `tests/lib/knowledge/` directory structure
2. Write unit tests for existing functionality (baseline):
   - `test_config_aware_filter.py`
   - `test_csv_hot_reload.py`
   - `test_knowledge_factory.py`
   - `test_row_based_csv.py`
3. Create test fixtures and mock data
4. Establish coverage baseline (target > 85%)

**Validation**: `uv run pytest tests/lib/knowledge/ --cov=lib.knowledge --cov-report=term-missing`

## ğŸ”„ Phase 2: PARALLEL Implementation - New Alongside Old

### T2.0: Extract Repository Layer (NEW - PARALLEL SAFE)
**Owner**: hive-dev-coder  
**Dependencies**: T1.0, T1.1  
**Parallel**: No - Foundation for other extractions  
**Strategy**: CREATE NEW, DON'T MODIFY OLD  

#### Create `lib/knowledge/repositories/knowledge_repository.py` (NEW FILE - DOESN'T TOUCH EXISTING):
```python
class KnowledgeRepository:
    """Handles all database operations for knowledge base"""
    
    def __init__(self, connection_pool):
        self._pool = connection_pool
    
    async def get_by_hash(self, content_hash: str) -> Optional[KnowledgeRecord]:
        """Retrieve record by content hash"""
    
    async def get_all_hashes(self) -> Set[str]:
        """Get all content hashes from database"""
    
    async def bulk_insert(self, records: List[KnowledgeRecord]) -> int:
        """Bulk insert records with proper transaction handling"""
    
    async def bulk_update(self, records: List[KnowledgeRecord]) -> int:
        """Bulk update existing records"""
    
    async def delete_by_hashes(self, hashes: Set[str]) -> int:
        """Delete records by content hash"""
```

### T2.1: Extract Data Source Layer
**Owner**: hive-dev-coder  
**Dependencies**: T1.0  
**Parallel**: Yes - Can run with T2.0  

#### Create `lib/knowledge/datasources/csv_datasource.py`:
```python
class CSVDataSource:
    """Handles CSV file operations and parsing"""
    
    def __init__(self, file_path: Path, config: CSVConfig):
        self._path = file_path
        self._config = config
    
    def read_rows(self) -> Iterator[Dict[str, Any]]:
        """Yield rows from CSV file"""
    
    def get_headers(self) -> List[str]:
        """Get CSV headers"""
    
    def validate_structure(self) -> ValidationResult:
        """Validate CSV structure against expected schema"""
```

### T2.2: Extract Hash Calculator Service
**Owner**: hive-dev-coder  
**Dependencies**: T1.0  
**Parallel**: Yes - Can run with T2.0, T2.1  

#### Create `lib/knowledge/services/hash_calculator.py`:
```python
class HashCalculator:
    """Handles content hashing and comparison"""
    
    def calculate_content_hash(self, content: str) -> str:
        """Calculate SHA256 hash of content"""
    
    def calculate_row_hash(self, row: Dict[str, Any], columns: List[str]) -> str:
        """Calculate hash for CSV row based on specified columns"""
    
    def compare_hashes(self, set_a: Set[str], set_b: Set[str]) -> HashComparison:
        """Compare two sets of hashes and return differences"""
```

### T2.3: Extract Change Analyzer Service
**Owner**: hive-dev-coder  
**Dependencies**: T2.2  
**Parallel**: No - Depends on hash calculator  

#### Create `lib/knowledge/services/change_analyzer.py`:
```python
class ChangeAnalyzer:
    """Analyzes changes between CSV and database state"""
    
    def __init__(self, hash_calculator: HashCalculator):
        self._hasher = hash_calculator
    
    def analyze_changes(self, csv_data: List[Dict], db_hashes: Set[str]) -> ChangeSet:
        """Analyze differences between CSV and database"""
        # Returns: added, modified, deleted record sets
    
    def generate_update_plan(self, changes: ChangeSet) -> UpdatePlan:
        """Create execution plan for database updates"""
```

### T2.4: Create NEW IncrementalLoader (PARALLEL SAFE)
**Owner**: hive-dev-coder  
**Dependencies**: T2.0, T2.1, T2.2, T2.3  
**Parallel**: No - Requires all extracted components  
**Strategy**: NEW CLASS - OLD SmartIncrementalLoader UNTOUCHED  

#### Create NEW `lib/knowledge/loaders/incremental_loader.py`:
```python
class IncrementalLoader:  # NEW class, doesn't replace old yet
    """Coordinates incremental knowledge base updates"""
    
    def __init__(
        self,
        repository: KnowledgeRepository,
        datasource: CSVDataSource,
        change_analyzer: ChangeAnalyzer,
        knowledge_base: KnowledgeBase
    ):
        # Dependency injection instead of instantiation
        self._repo = repository
        self._source = datasource
        self._analyzer = change_analyzer
        self._kb = knowledge_base
    
    async def load(self) -> LoadResult:
        """Orchestrate incremental loading process"""
        # Simplified coordination logic only
        # No direct SQL, file I/O, or hashing
```

**Target**: Reduce from 751 lines to < 200 lines

### ğŸ”„ T2.5: Validation & Checkpoint Commit
**Owner**: hive-qa-tester  
**Dependencies**: T2.0-T2.4  
**Parallel**: No - Critical validation point  

#### Tasks:
1. **Parallel validation**:
   ```python
   # Run both old and new implementations
   old_result = SmartIncrementalLoader().load()
   new_result = IncrementalLoader().load()
   assert old_result == new_result  # Must produce identical results
   ```

2. **Create checkpoint commit**:
   ```bash
   git add lib/knowledge/repositories/
   git add lib/knowledge/datasources/
   git add lib/knowledge/services/
   git add lib/knowledge/loaders/
   git commit -m "feat(knowledge): parallel implementation of refactored components (safe - old code untouched)"
   git tag checkpoint-phase-2-complete
   ```

3. **Rollback procedure documented**:
   ```bash
   # If issues detected:
   git reset --hard checkpoint-phase-2-complete
   # Or revert to previous checkpoint:
   git reset --hard checkpoint-phase-1-complete
   ```

## ğŸ”„ Phase 3: Gradual Migration & Architecture Enhancement

### T3.0: Implement Feature Flag Migration System
**Owner**: hive-dev-coder  
**Dependencies**: T2.5 checkpoint  
**Parallel**: No  
**Strategy**: GRADUAL SWITCHOVER  

#### Update `lib/knowledge/knowledge_factory.py` WITH FLAGS:
```python
def get_knowledge_base():
    """Factory with migration flags"""
    config = load_knowledge_config()
    
    if config.get('migration_flags.use_new_loader'):
        # Use new implementation
        return IncrementalLoader(...)
    else:
        # Keep using old implementation
        return SmartIncrementalLoader(...)
```

### T3.1: Implement Dependency Injection (NEW - SAFE)
**Owner**: hive-dev-designer  
**Dependencies**: T3.0  
**Parallel**: No  

#### Create `lib/knowledge/container.py`:
```python
class KnowledgeContainer:
    """Dependency injection container"""
    
    def __init__(self, config: KnowledgeConfig):
        self._config = config
        self._instances = {}
    
    def get_repository(self) -> KnowledgeRepository:
        """Get or create repository instance"""
    
    def get_incremental_loader(self) -> IncrementalLoader:
        """Get fully configured loader with dependencies"""
```

### T3.1: Replace Singleton Pattern
**Owner**: hive-dev-coder  
**Dependencies**: T3.0  
**Parallel**: No  

#### Refactor `knowledge_factory.py`:
```python
class KnowledgeFactory:
    """Factory for creating knowledge base instances"""
    
    def __init__(self, container: KnowledgeContainer):
        self._container = container
    
    def create_shared_instance(self) -> KnowledgeBase:
        """Create properly configured shared instance"""
        # No global state or manual locking
```

### T3.2: Implement Configuration Manager
**Owner**: hive-dev-coder  
**Dependencies**: T3.0  
**Parallel**: Yes - Can run with T3.1  

#### Create `lib/knowledge/config/manager.py`:
```python
class ConfigurationManager:
    """Centralized configuration management"""
    
    def __init__(self, config_path: Path):
        self._path = config_path
        self._config = self._load_config()
    
    def get_knowledge_config(self) -> KnowledgeConfig:
        """Get knowledge-specific configuration"""
    
    def get_business_units(self) -> Dict[str, BusinessUnit]:
        """Get business unit configurations"""
    
    # Single source of truth for all configuration
```

### T3.3: Add Error Handling Layer
**Owner**: hive-dev-coder  
**Dependencies**: Phase 2 complete  
**Parallel**: Yes  

#### Create `lib/knowledge/exceptions.py`:
```python
class KnowledgeBaseError(Exception):
    """Base exception for knowledge system"""

class ConfigurationError(KnowledgeBaseError):
    """Configuration-related errors"""

class DataSourceError(KnowledgeBaseError):
    """Data source access errors"""

class ValidationError(KnowledgeBaseError):
    """Data validation errors"""

class RepositoryError(KnowledgeBaseError):
    """Database operation errors"""
```

## ğŸ¨ Phase 4: Quality & Standards Compliance

### T4.0: Fix All Naming Violations
**Owner**: hive-quality-ruff  
**Dependencies**: Phase 3 complete  
**Parallel**: No  

#### Naming Standards:
1. **Methods**: `snake_case`, verb prefixes (`get_`, `set_`, `calculate_`, `validate_`)
2. **Classes**: `PascalCase`, noun phrases
3. **Constants**: `UPPER_SNAKE_CASE`
4. **Private**: Leading underscore `_private_method`
5. **Files**: `snake_case.py`, descriptive module names

**Validation**: `uv run ruff check lib/knowledge/ --select N`

### T4.1: Add Complete Type Hints
**Owner**: hive-quality-mypy  
**Dependencies**: T4.0  
**Parallel**: No  

#### Type Annotation Requirements:
- All function signatures with parameter and return types
- Use `typing` module for complex types
- Create type aliases for commonly used types
- Add `py.typed` marker file

**Validation**: `uv run mypy lib/knowledge/ --strict`

### T4.2: Add Comprehensive Documentation
**Owner**: hive-dev-coder  
**Dependencies**: T4.0  
**Parallel**: Yes - Can run with T4.1  

#### Documentation Standards:
```python
def process_knowledge(
    self,
    data: List[Dict[str, Any]],
    config: ProcessConfig
) -> ProcessResult:
    """Process knowledge data according to configuration.
    
    Args:
        data: List of dictionaries containing knowledge entries.
            Each entry must have 'id', 'content', and 'metadata' keys.
        config: Processing configuration specifying filters and transforms.
    
    Returns:
        ProcessResult containing processed entries and statistics.
    
    Raises:
        ValidationError: If data format is invalid.
        ConfigurationError: If config is malformed.
    
    Example:
        >>> processor = KnowledgeProcessor()
        >>> result = processor.process_knowledge(
        ...     data=[{"id": "1", "content": "...", "metadata": {}}],
        ...     config=ProcessConfig(filters=["active"])
        ... )
        >>> print(f"Processed {result.count} entries")
    """
```

### T4.3: Performance Optimization
**Owner**: hive-dev-coder  
**Dependencies**: Phase 3 complete  
**Parallel**: Yes  

#### Optimization Tasks:
1. **Eliminate temporary file creation**:
   - Process single rows in memory
   - Use `io.StringIO` for CSV operations
   
2. **Optimize database queries**:
   - Use batch operations with proper sizing
   - Implement connection pooling
   - Add query result caching where appropriate
   
3. **Improve hashing performance**:
   - Cache computed hashes
   - Use faster hashing for non-cryptographic needs

**Validation**: Performance benchmarks showing >50% improvement

## âœ… Phase 5: Validation & Integration Testing

### T5.0: Integration Test Suite
**Owner**: hive-testing-maker  
**Dependencies**: Phase 4 complete  
**Parallel**: No  

#### Test Coverage Requirements:
```python
# tests/lib/knowledge/integration/test_end_to_end.py
class TestKnowledgeSystemIntegration:
    """End-to-end integration tests"""
    
    def test_full_incremental_load_cycle(self):
        """Test complete load, update, delete cycle"""
    
    def test_concurrent_access(self):
        """Test thread-safe shared instance access"""
    
    def test_large_csv_processing(self):
        """Test with 10K+ row CSV files"""
    
    def test_error_recovery(self):
        """Test graceful handling of failures"""
```

### T5.1: Security Validation
**Owner**: hive-qa-tester  
**Dependencies**: T5.0  
**Parallel**: Yes  

#### Security Tests:
- SQL injection attempts
- Path traversal attempts  
- CSV injection vectors
- Resource exhaustion scenarios

### T5.2: Performance Validation
**Owner**: hive-qa-tester  
**Dependencies**: T5.0  
**Parallel**: Yes - Can run with T5.1  

#### Performance Benchmarks:
- Load time for 10K records: < 5 seconds
- Memory usage: < 500MB for large datasets
- Query response time: < 100ms p95
- Hot reload detection: < 1 second

### T5.3: Final Quality Gates
**Owner**: hive-dev-fixer  
**Dependencies**: T5.0, T5.1, T5.2  
**Parallel**: No - Final validation  

#### Checklist:
- [ ] All tests passing (`uv run pytest`)
- [ ] Zero mypy errors (`uv run mypy . --strict`)
- [ ] Zero ruff violations (`uv run ruff check .`)
- [ ] Test coverage > 85%
- [ ] All documentation complete
- [ ] Performance benchmarks met
- [ ] Security scan clean

## ğŸ¯ Orchestration Strategy

### Execution Plan
```python
# Master Genie Orchestration Commands

# PHASE 1: Foundation (Sequential)
Task(subagent_type="hive-dev-fixer", prompt="Execute T1.0: Remove all dead code from lib/knowledge/")
Task(subagent_type="hive-testing-maker", prompt="Execute T1.1: Create test infrastructure for lib/knowledge/")

# PHASE 2: Core Refactoring (Parallel where possible)
# Parallel batch 1
Task(subagent_type="hive-dev-coder", prompt="Execute T2.0: Extract KnowledgeRepository from SmartIncrementalLoader")
Task(subagent_type="hive-dev-coder", prompt="Execute T2.1: Extract CSVDataSource from SmartIncrementalLoader")
Task(subagent_type="hive-dev-coder", prompt="Execute T2.2: Extract HashCalculator service")

# Sequential after batch 1
Task(subagent_type="hive-dev-coder", prompt="Execute T2.3: Extract ChangeAnalyzer service using HashCalculator")
Task(subagent_type="hive-dev-coder", prompt="Execute T2.4: Refactor SmartIncrementalLoader using all extracted components")

# PHASE 3: Architecture (Mixed)
Task(subagent_type="hive-dev-designer", prompt="Execute T3.0: Design and implement dependency injection container")
# Parallel batch
Task(subagent_type="hive-dev-coder", prompt="Execute T3.1: Replace singleton pattern in knowledge_factory")
Task(subagent_type="hive-dev-coder", prompt="Execute T3.2: Implement ConfigurationManager")
Task(subagent_type="hive-dev-coder", prompt="Execute T3.3: Add comprehensive error handling layer")

# PHASE 4: Quality (Sequential for consistency)
Task(subagent_type="hive-quality-ruff", prompt="Execute T4.0: Fix all naming violations in lib/knowledge/")
Task(subagent_type="hive-quality-mypy", prompt="Execute T4.1: Add complete type hints to lib/knowledge/")
# Parallel documentation and optimization
Task(subagent_type="hive-dev-coder", prompt="Execute T4.2: Add comprehensive documentation")
Task(subagent_type="hive-dev-coder", prompt="Execute T4.3: Performance optimization")

# PHASE 5: Validation (Parallel testing)
Task(subagent_type="hive-testing-maker", prompt="Execute T5.0: Create integration test suite")
# Parallel validation
Task(subagent_type="hive-qa-tester", prompt="Execute T5.1: Security validation testing")
Task(subagent_type="hive-qa-tester", prompt="Execute T5.2: Performance validation benchmarks")
# Final gate
Task(subagent_type="hive-dev-fixer", prompt="Execute T5.3: Final quality gates validation")
```

### Agent Coordination Matrix
| Phase | Lead Agent | Support Agents | Parallel Capacity |
|-------|------------|----------------|-------------------|
| Phase 1 | hive-dev-fixer | hive-testing-maker | Limited |
| Phase 2 | hive-dev-coder | - | 3 parallel tasks |
| Phase 3 | hive-dev-designer | hive-dev-coder | 3 parallel tasks |
| Phase 4 | hive-quality-ruff | hive-quality-mypy, hive-dev-coder | 2 parallel tasks |
| Phase 5 | hive-testing-maker | hive-qa-tester, hive-dev-fixer | 2 parallel tasks |

### Context Provision Requirements
Each agent receives:
1. Full task specification from this document
2. Dependencies list and completion status
3. File paths and specific line numbers
4. Success criteria and validation commands
5. Integration points with other components

## ğŸ“Š Progress Tracking

### Milestone Checkpoints
- [ ] **Milestone 1**: Dead code eliminated, tests created (T1.0-T1.1)
- [ ] **Milestone 2**: God class refactored (T2.0-T2.4)  
- [ ] **Milestone 3**: Architecture enhanced (T3.0-T3.3)
- [ ] **Milestone 4**: Quality standards met (T4.0-T4.3)
- [ ] **Milestone 5**: Full validation complete (T5.0-T5.3)

### Risk Mitigation
1. **Breaking changes**: Each phase includes comprehensive testing
2. **Performance regression**: Benchmarks at each milestone
3. **Lost functionality**: Integration tests verify all features
4. **Coordination failures**: Clear dependency management

## ğŸ† Completion Criteria

The wish is fulfilled when:
1. âœ… All 5 phases complete
2. âœ… Zero dead code remains
3. âœ… Zero naming violations  
4. âœ… All files < 350 lines
5. âœ… Test coverage > 85%
6. âœ… All quality gates passed
7. âœ… Performance benchmarks met
8. âœ… Security validation clean

## ğŸš€ DEATH TESTAMENT

Upon successful completion, this wish document will be updated with:
- Final metrics and benchmarks
- Lessons learned during refactoring  
- Architecture decision records
- Performance improvement statistics
- Test coverage reports
- Security audit results

---

**Wish Status**: ğŸ“‹ Ready for Execution  
**Next Action**: Approve phase-by-phase execution plan  
**Estimated Total Execution**: Sequential phases with parallel optimization where possible