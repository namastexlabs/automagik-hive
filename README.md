# Automagik Hive

Enterprise multi-agent AI framework built on **Agno** that enables rapid development of sophisticated multi-agent systems through YAML configuration. Production-ready boilerplate for building intelligent agents, routing teams, and business workflows with enterprise-grade deployment capabilities.

## üöÄ Quick Start

```bash
# Install dependencies
uv sync

# Setup agent environment (Docker containers)
make install-agent

# Start agent services
make agent

# Your isolated agent environment:
# - Agent API: http://localhost:38886
# - Agent DB: postgresql://localhost:35532
```

---

## üìä ProcessamentoFaturas Workflow

**Automated CTE Invoice Processing Pipeline** - A comprehensive workflow that automates the complete lifecycle of CTE (Conhecimento de Transporte Eletr√¥nico) invoice processing from email monitoring to API orchestration.

### üìã **VIS√ÉO GERAL ARQUITETURAL**

O workflow √© um pipeline de 5 steps sequenciais que processa CTEs (Conhecimento de Transporte Eletr√¥nico) de forma automatizada e resiliente, com execu√ß√£o di√°ria programada.

#### üîÑ **FLUXO COMPLETO DE DADOS:**
```
üìß Gmail ‚Üí üìä Excel ‚Üí üìÑ JSON ‚Üí üîç An√°lise ‚Üí üéØ Routing ‚Üí ‚öôÔ∏è APIs ‚Üí üíæ Update ‚Üí üèÅ Complete
   ‚Üì         ‚Üì         ‚Üì         ‚Üì          ‚Üì         ‚Üì         ‚Üì         ‚Üì
STEP 1    STEP 1    STEP 1    STEP 2    STEP 3    STEP 4    STEP 5    STEP 5
```

#### üé™ **CARACTER√çSTICAS ARQUITETURAIS:**
- **‚úÖ RESILIENTE**: Retry autom√°tico, estado persistente, recupera√ß√£o de falhas  
- **‚úÖ ESCAL√ÅVEL**: Processamento batch + individual otimizado  
- **‚úÖ INCREMENTAL**: Continuidade entre execu√ß√µes di√°rias  
- **‚úÖ OBSERV√ÅVEL**: Logging detalhado, m√©tricas completas  
- **‚úÖ CONFIGUR√ÅVEL**: Timeouts, limites, URLs configur√°veis  

#### üìà **PERFORMANCE ESPERADA:**
- **Throughput**: ~22 POs processados em 15 minutos
- **Lat√™ncia**: APIs com timeout de 15 minutos  
- **Resili√™ncia**: 3 tentativas com exponential backoff
- **Limite**: 3 emails por execu√ß√£o di√°ria

#### üîÆ **CICLO DE VIDA COMPLETO DE UM PO:**
```
DIA 1: Email ‚Üí Excel ‚Üí JSON ‚Üí PO status: PENDING ‚Üí invoiceGen ‚Üí WAITING_MONITORING
DIA 2: JSON analysis ‚Üí PO status: WAITING_MONITORING ‚Üí invoiceMonitor ‚Üí MONITORED  
DIA 3: JSON analysis ‚Üí PO status: MONITORED ‚Üí download ‚Üí DOWNLOADED
DIA 4: JSON analysis ‚Üí PO status: DOWNLOADED ‚Üí upload ‚Üí UPLOADED ‚úÖ
```

### üéØ Vis√£o Geral

O workflow ProcessamentoFaturas √© um **sistema programado di√°rio** que transforma o processamento manual de faturas CTE em um pipeline automatizado, baseado em status com rastreamento individual de POs e processamento incremental:

1. **üåÖ Inicializa√ß√£o Di√°ria** - Busca dupla por novos emails E arquivos JSON existentes com processamento de backlog
2. **üîç An√°lise JSON** - Extra√ß√£o e categoriza√ß√£o de status individual de POs de todos os arquivos JSON
3. **üéØ Roteamento Baseado em Status** - Roteamento inteligente de cada PO para o step de processamento apropriado baseado no status atual
4. **‚öôÔ∏è Processamento Individual de PO** - Chamadas de API espec√≠ficas por status com modos de processamento em lote e individual
5. **üèÅ Finaliza√ß√£o Di√°ria** - Atualiza√ß√µes de arquivos JSON, persist√™ncia de status e agendamento da pr√≥xima execu√ß√£o

### üîÑ **Mudan√ßa Arquitetural Revolucion√°ria**

**De**: Execu√ß√£o linear √∫nica (Email ‚Üí Dados ‚Üí JSON ‚Üí API ‚Üí Completo)  
**Para**: Processamento c√≠clico di√°rio com rastreamento de status individual de PO

#### **üöÄ Principais Benef√≠cios da Transforma√ß√£o:**
- **Execu√ß√£o Programada Di√°ria**: Executa automaticamente todos os dias √†s 8h
- **Processamento Incremental**: Cada PO avan√ßa gradualmente atrav√©s do pipeline ao longo de m√∫ltiplos dias
- **Rastreamento de Status Individual**: Cada PO mant√©m seu pr√≥prio status de processamento em arquivos JSON
- **Gerenciamento de Backlog**: Processa POs pendentes de execu√ß√µes anteriores junto com novos emails
- **Processamento Resiliente**: Falhas individuais de PO n√£o afetam outros POs no lote
- **Roteamento Inteligente**: Cada PO √© roteado exatamente para o step de processamento que precisa

### üèóÔ∏è Arquitetura

#### **Tipo de Workflow**: Agno Workflows 2.0 - Arquitetura Programada Di√°ria
Constru√≠do usando **execu√ß√£o c√≠clica di√°ria** com processamento de PO baseado em status e agentes especializados:

```python
Workflow(
    name="processamento_faturas",
    description="Processamento di√°rio programado de faturas CTE com roteamento baseado em status individual de PO",
    steps=[
        Step("daily_initialization", executor=execute_daily_initialization_step),
        Step("json_analysis", executor=execute_json_analysis_step),
        Step("status_based_routing", executor=execute_status_based_routing_step), 
        Step("individual_po_processing", executor=execute_individual_po_processing_step),
        Step("daily_completion", executor=execute_daily_completion_step)
    ],
    storage=PostgresStorage(auto_upgrade_schema=True)
)
```

#### **üîÑ Exemplo de Ciclo de Processamento Di√°rio:**
```
Dia 1 (8h): Busca email ‚Üí 3 novos POs (PENDING) ‚Üí invoiceGen ‚Üí 3 POs (WAITING_MONITORING)
Dia 2 (8h): An√°lise JSON ‚Üí 3 POs (WAITING_MONITORING) ‚Üí invoiceMonitor ‚Üí 3 POs (MONITORED)
Dia 3 (8h): An√°lise JSON ‚Üí 3 POs (MONITORED) ‚Üí downloads individuais ‚Üí 3 POs (DOWNLOADED)
Dia 4 (8h): An√°lise JSON ‚Üí 3 POs (DOWNLOADED) ‚Üí uploads individuais ‚Üí 3 POs (UPLOADED) ‚úÖ
```

#### **5 Agentes Especializados - Aprimorados para Processamento Di√°rio**
Cada step utiliza agentes dedicados com expertise espec√≠fica de processamento di√°rio:

- **üìß EmailProcessor** - Gmail OAuth2 + busca de arquivo JSON existente, processamento matutino de email
- **üìä DataExtractor** - An√°lise de arquivo JSON, extra√ß√£o de status individual de PO, categoriza√ß√£o
- **üèóÔ∏è JSONGenerator** - L√≥gica de roteamento baseada em status, organiza√ß√£o de fila de processamento  
- **üîó APIOrchestrator** - Chamadas de API individual de PO, modos de processamento em lote vs individual
- **üìÅ FileManager** - Atualiza√ß√µes de arquivo JSON, persist√™ncia de status, agendamento de pr√≥xima execu√ß√£o

#### **üéØ L√≥gica de Processamento Baseada em Status:**
```
POs PENDING        ‚Üí API invoiceGen (processamento em lote)    ‚Üí WAITING_MONITORING
WAITING_MONITORING ‚Üí API invoiceMonitor (lote)               ‚Üí MONITORED  
POs MONITORED      ‚Üí API download (individual por PO)        ‚Üí DOWNLOADED
POs DOWNLOADED     ‚Üí API upload (individual por PO)          ‚Üí UPLOADED ‚úÖ
POs UPLOADED       ‚Üí Pular (j√° completado)                   ‚Üí Nenhuma a√ß√£o
POs FAILED_*       ‚Üí Tratamento de erro e l√≥gica de retry    ‚Üí Recupera√ß√£o espec√≠fica por status
```

### üìã Fluxo de Dados

#### **Entrada**: Conta Gmail com Anexos Excel CTE
- Acesso Gmail autenticado OAuth2
- Arquivos Excel contendo entradas CTE e MINUTA
- Valida√ß√£o e download automatizado de anexos

#### **Processamento**: Extra√ß√£o e Valida√ß√£o de CTE
```json
{
  "600708542": {
    "values": [
      {
        "index": 154,
        "NF/CTE": "96765", 
        "valor CHAVE": 1644.67,
        "Empresa Origem": "CLARO MOVEL ENG",
        "CNPJ Fornecedor": "9351138000100",
        "Compet√™ncia": "45778"
      }
    ],
    "total_value": 1644.67,
    "cte_count": 1,
    "status": "PENDING"
  }
}
```

#### **Sa√≠da**: Estrutura JSON Di√°ria com Rastreamento de Status
```json
{
  "batch_id": "daily_20250113_080012",
  "source_file": "faturas_janeiro_2025.xlsx",
  "processing_timestamp": "2025-01-13T08:00:12Z",
  "last_daily_update": "2025-01-13T08:15:32Z",
  "total_ctes": 15,
  "total_pos": 8,
  "total_value": 45230.89,
  "client_data": ["CLARO MOVEL ENG", 9351138000100],
  "type": "CTE",
  "orders": [
    {
      "po_number": "600708542",
      "cte_entries": [...],
      "total_value": 1644.67,
      "status": "WAITING_MONITORING",  // Rastreamento de status individual de PO
      "last_updated": "2025-01-13T08:15:32Z",
      "processing_history": [
        {"status": "PENDING", "timestamp": "2025-01-12T08:00:00Z"},
        {"status": "WAITING_MONITORING", "timestamp": "2025-01-13T08:15:32Z"}
      ]
    },
    {
      "po_number": "600708543", 
      "cte_entries": [...],
      "total_value": 2189.45,
      "status": "DOWNLOADED",  // POs diferentes podem estar em diferentes est√°gios
      "last_updated": "2025-01-12T08:20:15Z",
      "download_path": "mctech/downloads/fatura_600708543.pdf"
    }
  ]
}
```

### üîó Integra√ß√£o com Browser API

#### **Orquestra√ß√£o de API Baseada em Status**
O workflow di√°rio se integra com a Browser API atrav√©s de execu√ß√£o de fluxo orientada por status usando um **endpoint √∫nico**:

**üéØ Endpoint Unificado**: `POST /execute_flow`

Todas as chamadas de API usam o mesmo endpoint com diferentes par√¢metros `flow_name`:

1. **`invoiceGen`** - **Processamento em lote** para POs PENDING (m√∫ltiplos POs por chamada)
2. **`invoiceMonitor`** - **Processamento em lote** para POs WAITING_MONITORING  
3. **`main-download-invoice`** - **Processamento individual** para POs MONITORED (uma chamada API por PO)
4. **`invoiceUpload`** - **Processamento individual** para POs DOWNLOADED (uma chamada API por PO)

#### **üìã Estrutura de Chamada API**
```http
POST http://localhost:8088/execute_flow
Content-Type: application/json

{
  "flow_name": "invoiceGen",
  "parameters": {
    "orders": ["600708542", "600708543"],
    "headless": true
  },
  "headless": true
}
```

#### **üîÑ Distribui√ß√£o de Chamadas API Di√°rias:**
```
Processos de Execu√ß√£o Di√°ria √önica:
‚îú‚îÄ‚îÄ 5 POs PENDING      ‚Üí 1 chamada invoiceGen em lote    ‚Üí 5 POs WAITING_MONITORING
‚îú‚îÄ‚îÄ 3 POs MONITORING   ‚Üí 1 chamada invoiceMonitor em lote ‚Üí 3 POs MONITORED  
‚îú‚îÄ‚îÄ 2 POs MONITORED    ‚Üí 2 chamadas download individuais  ‚Üí 2 POs DOWNLOADED
‚îî‚îÄ‚îÄ 4 POs DOWNLOADED   ‚Üí 4 chamadas upload individuais    ‚Üí 4 POs UPLOADED ‚úÖ

Total de chamadas API: 8 chamadas processando 14 POs em v√°rios est√°gios
```

#### **Formato de Resposta da API (API_RESULT)**
```json
{
  "success": true,
  "data": {
    "job_id": "job_12345",
    "status": "processing",
    "message": "Gera√ß√£o de fatura iniciada"
  },
  "error": null,
  "timestamp": "2025-01-12T14:30:52Z"
}
```

#### **Tratamento de Erros**
```json
{
  "success": false,
  "data": null,
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Formato de PO inv√°lido",
    "details": "N√∫mero do PO deve ser num√©rico"
  },
  "timestamp": "2025-01-12T14:30:52Z"
}
```

### üíæ Gerenciamento de Estado - Arquitetura de Persist√™ncia Di√°ria

#### **Persist√™ncia PostgreSQL Aprimorada**
- **Estado de Sess√£o Di√°rio**: Compartilhamento de dados entre steps com contexto de execu√ß√£o di√°ria
- **Status Individual de PO**: Rastreamento de status persistente por PO ao longo de m√∫ltiplos dias
- **Estado de Arquivo JSON**: Atualiza√ß√µes de status em tempo real escritas de volta aos arquivos JSON
- **Gerenciamento de Backlog**: Identifica√ß√£o autom√°tica e processamento de POs pendentes
- **Recupera√ß√£o de Erro**: Isolamento de falha individual de PO com recupera√ß√£o espec√≠fica por status
- **M√©tricas Di√°rias**: Rastreamento de performance ao longo de m√∫ltiplas execu√ß√µes di√°rias

#### **Fluxo de Transi√ß√£o de Status Di√°rio**
```
üîÑ CICLO DE EXECU√á√ÉO DI√ÅRIA:

Dia N-2: PENDING ‚Üí invoiceGen (lote) ‚Üí WAITING_MONITORING
Dia N-1: WAITING_MONITORING ‚Üí invoiceMonitor (lote) ‚Üí MONITORED  
Dia N:   MONITORED ‚Üí download (individual) ‚Üí DOWNLOADED
Dia N+1: DOWNLOADED ‚Üí upload (individual) ‚Üí UPLOADED ‚úÖ

üìä PROCESSAMENTO PARALELO:
- M√∫ltiplos POs podem estar em diferentes est√°gios simultaneamente
- Cada PO progride independentemente baseado em seu status individual
- POs falhados n√£o bloqueiam progress√£o de POs bem-sucedidos
- Novos POs de emails s√£o adicionados √† fila PENDING diariamente

‚ùå TRATAMENTO DE ERROS:
FAILED_EXTRACTION   ‚Üí Tentar novamente processamento email/Excel (pr√≥ximo dia)
FAILED_GENERATION   ‚Üí Tentar novamente chamada API invoiceGen
FAILED_MONITORING   ‚Üí Tentar novamente chamada API invoiceMonitor  
FAILED_DOWNLOAD     ‚Üí Tentar novamente download individual
FAILED_UPLOAD       ‚Üí Tentar novamente upload individual
```

### üß™ Testes

#### **Su√≠te de Testes Abrangente (70+ Casos de Teste)**
```bash
# Executar su√≠te de testes completa
uv run pytest ai/workflows/processamento-faturas/tests.py -v

# Categorias de teste:
# - Testes Unit√°rios: Valida√ß√£o de n√≠vel de componente
# - Testes de Integra√ß√£o: Valida√ß√£o de intera√ß√£o de agente  
# - Testes End-to-End: Execu√ß√£o completa de workflow
# - Testes de Performance: Conformidade de tempo de resposta
# - Cen√°rios de Erro: Valida√ß√£o de tratamento de falha
```

#### **Metodologia TDD**
- Conformidade de ciclo **Red-Green-Refactor**
- Abordagem de implementa√ß√£o **test-first**
- **Estrat√©gias de mock** para depend√™ncias externas
- **Metas de cobertura**: >90% de cobertura de c√≥digo

### ‚ö° Metas de Performance - Otimizado para Execu√ß√£o Di√°ria

- **Inicializa√ß√£o Di√°ria**: <10s para busca de email + an√°lise de arquivo JSON
- **An√°lise JSON**: <5s por arquivo JSON existente analisado
- **Roteamento Baseado em Status**: <3s para organiza√ß√£o de fila de processamento
- **Opera√ß√µes de API em Lote**: <15s por chamada em lote (invoiceGen, invoiceMonitor)
- **Opera√ß√µes de API Individual**: <8s por chamada individual (download, upload)
- **Finaliza√ß√£o Di√°ria**: <5s para atualiza√ß√µes de arquivo JSON e agendamento
- **Execu√ß√£o Di√°ria Total**: <2 minutos para carga de trabalho mista t√≠pica

#### **üìä Capacidade de Processamento Di√°rio:**
```
Carga de Trabalho Di√°ria T√≠pica:
‚îú‚îÄ‚îÄ Novos emails: 0-3 (apenas manh√£, <12h)
‚îú‚îÄ‚îÄ POs existentes para processar: 10-50 em v√°rios est√°gios
‚îú‚îÄ‚îÄ Chamadas API em lote: 0-2 (invoiceGen + invoiceMonitor)
‚îú‚îÄ‚îÄ Chamadas API individual: 5-25 (downloads + uploads)
‚îî‚îÄ‚îÄ Arquivos JSON atualizados: 3-8 arquivos consolidados

Performance: Processa 50+ POs em m√∫ltiplos est√°gios em menos de 2 minutos
```

### üîß Configura√ß√£o

#### **Configura√ß√£o de Ambiente**
```bash
# Vari√°veis de ambiente obrigat√≥rias (.env)
GMAIL_CLIENT_ID=your_gmail_oauth_client_id
GMAIL_CLIENT_SECRET=your_gmail_oauth_secret

# Conex√£o com banco de dados (configurada automaticamente com make agent)
DATABASE_URL="postgresql://localhost:35532/hive_agent"

# Configura√ß√£o da Browser API
BROWSER_API_BASE_URL="http://localhost:8088"
BROWSER_API_TIMEOUT="900"
BROWSER_API_MAX_RETRIES="3"
```

#### **üìã Requisitos do Servidor Browser API**
O workflow requer um **servidor Browser API ativo** em `http://localhost:8088`:

```bash
# Browser API deve estar executando e respondendo a:
curl -X POST http://localhost:8088/execute_flow \
  -H "Content-Type: application/json" \
  -d '{"flow_name": "test", "parameters": {}}'

# Verifica√ß√£o de sa√∫de (se dispon√≠vel):
curl http://localhost:8088/health
```

**‚ö†Ô∏è Importante**: O workflow **falhar√° imediatamente** se a Browser API n√£o estiver dispon√≠vel. Certifique-se de que o servidor esteja executando antes de executar o workflow.

#### **Configura√ß√£o de Agentes** 
```yaml
# ai/workflows/processamento-faturas/config.yaml
name: "processamento_faturas"
version: "1.0.0"
description: "CTE Invoice Processing Pipeline"
agents:
  email_processor:
    model: "claude-3-sonnet"
    temperature: 0.2
    tools: ["gmail_oauth", "file_validator"]
  data_extractor:
    model: "claude-3-sonnet"  
    temperature: 0.1
    tools: ["excel_processor", "data_validator"]
```

### üöÄ Implanta√ß√£o - Execu√ß√£o Programada Di√°ria

#### **Desenvolvimento Local**
```bash
# Iniciar servi√ßo de workflow
uv run python api/main.py

# Acessar playground para testes manuais
open http://localhost:8000

# Testar execu√ß√£o di√°ria do workflow
cd ai/workflows/processamento-faturas/
uv run python workflow.py
```

#### **Implanta√ß√£o de Produ√ß√£o com Agendamento Di√°rio**
```bash
# Implanta√ß√£o Docker
docker build -t hive-workflows .
docker run -p 8000:8000 hive-workflows

# Configura√ß√£o de cron job para execu√ß√£o di√°ria √†s 8h
crontab -e
# Adicionar: 0 8 * * * cd /project && uv run python -c "
import asyncio
from ai.workflows.processamento_faturas.workflow import get_processamento_faturas_workflow

async def daily_run():
    workflow = get_processamento_faturas_workflow()
    await workflow.arun('Execute daily CTE processing')

asyncio.run(daily_run())
"
```

#### **‚ö° Execu√ß√£o R√°pida**
```bash
# 1. Garantir que a Browser API est√° executando
curl http://localhost:8088/health  # Deve retornar 200

# 2. Iniciar servi√ßos de agentes
make agent

# 3. Executar workflow via API Playground
uv run python api/main.py
# Acessar: http://localhost:8000
# Selecionar: processamento_faturas workflow -> Run
```

#### **üï∞Ô∏è Op√ß√µes de Agendamento:**
```bash
# Op√ß√£o 1: Cron Job (Linux/Mac) - com verifica√ß√£o de API
#!/bin/bash
# daily-workflow-runner.sh
if curl -sf http://localhost:8088/health > /dev/null; then
  cd /project && uv run python api/main.py --execute processamento_faturas
else
  echo "Browser API n√£o dispon√≠vel - workflow ignorado" >&2
  exit 1
fi

# Cron: 0 8 * * * /path/to/daily-workflow-runner.sh

# Op√ß√£o 2: Docker Compose com Agendador
services:
  browser-api:
    image: browser-api:latest
    ports: ["8088:8088"]
  
  hive-scheduler:
    build: .
    command: python daily_scheduler.py
    environment:
      - SCHEDULE_TIME=08:00
      - WORKFLOW_NAME=processamento_faturas
      - BROWSER_API_BASE_URL=http://browser-api:8088
    depends_on: [browser-api]

# Op√ß√£o 3: Execu√ß√£o agendada MCP automagik-hive
mcp__automagik_hive__schedule_workflow(
    workflow_id="processamento_faturas",
    schedule="0 8 * * *",  # Daily at 8 AM
    input_data={"mode": "daily_processing"}
)
```

### üìä Monitoramento e M√©tricas

#### **M√©tricas do Workflow Di√°rio**
- **Tempo de Execu√ß√£o Di√°rio**: Dura√ß√£o completa do ciclo di√°rio (meta <2 minutos)
- **Volume de Processamento de POs**: Total de POs processados em todas as etapas por dia
- **Taxa de Progress√£o de Status**: POs avan√ßando de um status para o pr√≥ximo por dia
- **Taxa de Sucesso da API**: Porcentagem de sucesso das chamadas Browser API por tipo de chamada
- **Redu√ß√£o do Backlog**: Taxa de conclus√£o de POs (status UPLOADED) por dia
- **Propor√ß√£o Novo vs Existente**: Novos emails processados vs backlog de POs existentes tratados

#### **Rastreamento Individual de PO**
- **Dura√ß√£o do Status**: Tempo gasto em cada est√°gio de processamento
- **Lat√™ncia Ponta a Ponta**: Dias de PENDING para UPLOADED por PO
- **Tempo de Recupera√ß√£o de Erro**: Tempo para recuperar de falhas individuais de PO
- **Performance Lote vs Individual**: Compara√ß√£o de efici√™ncia dos modos de processamento

#### **üìà M√©tricas do Dashboard Di√°rio:**
```
Resumo da Execu√ß√£o Di√°ria (2025-01-13):
‚îú‚îÄ‚îÄ Total POs Encontrados: 23
‚îú‚îÄ‚îÄ Novos POs Adicionados: 3 (dos emails da manh√£)
‚îú‚îÄ‚îÄ POs Processados: 18
‚îÇ   ‚îú‚îÄ‚îÄ Opera√ß√µes em Lote: 8 POs (2 chamadas API)
‚îÇ   ‚îî‚îÄ‚îÄ Opera√ß√µes Individuais: 10 POs (10 chamadas API)
‚îú‚îÄ‚îÄ POs Conclu√≠dos: 6 (alcan√ßaram status UPLOADED)
‚îú‚îÄ‚îÄ POs Restantes: 5 (v√°rias etapas)
‚îî‚îÄ‚îÄ Pr√≥xima Execu√ß√£o: 2025-01-14 08:00:00
```

### üîí Seguran√ßa e Conformidade

#### **Prote√ß√£o de Dados**
- **Seguran√ßa OAuth2**: Integra√ß√£o segura com Gmail com atualiza√ß√£o de token
- **Integridade de Arquivos**: Checksums SHA-256 para todos os arquivos processados
- **Criptografia de Estado**: Criptografia de dados sens√≠veis no PostgreSQL
- **Log de Auditoria**: Trilha completa de auditoria de processamento

#### **Recupera√ß√£o de Erros**
- **Backoff Exponencial**: Estrat√©gias de retry inteligentes para falhas de API
- **Persist√™ncia de Estado**: Recupera√ß√£o de qualquer falha de etapa
- **Degrada√ß√£o Graciosa**: Tratamento de sucesso parcial de processamento

### üõ†Ô∏è Desenvolvimento

#### **Adicionando Novas Funcionalidades**
```bash
# 1. Estender etapas do workflow
# Editar: ai/workflows/processamento-faturas/workflow.py

# 2. Adicionar testes correspondentes
# Editar: ai/workflows/processamento-faturas/tests.py

# 3. Atualizar configura√ß√£o
# Editar: ai/workflows/processamento-faturas/config.yaml

# 4. Executar verifica√ß√µes de qualidade
uv run ruff check --fix
uv run mypy .
uv run pytest
```

#### **Personaliza√ß√£o de Agentes**
```python
# Criar varia√ß√µes especializadas de agentes
def create_custom_extractor_agent():
    return Agent(
        name="Extrator de Dados Personalizado",
        instructions=["L√≥gica de extra√ß√£o personalizada"],
        tools=[custom_excel_tool],
        model=create_workflow_model()
    )
```

### üìö Refer√™ncias T√©cnicas

- **Framework**: [Documenta√ß√£o Agno](https://docs.agno.ai)
- **Padr√µes de Workflow**: [Agno Workflows 2.0](https://docs.agno.ai/workflows)
- **Arquitetura de Agentes**: [Sistemas Multi-Agente](https://docs.agno.ai/agents)
- **Armazenamento PostgreSQL**: [Agno Storage](https://docs.agno.ai/storage)

### üîß Detalhes de Implementa√ß√£o T√©cnica

#### **Execu√ß√£o T√©cnica Passo-a-Passo**

##### **üåÖ ETAPA 1: `execute_daily_initialization_step`**
**Fun√ß√£o**: `async def execute_daily_initialization_step(step_input: StepInput) -> StepOutput`

**Implementa√ß√£o Principal:**
```python
# Integra√ß√£o Real com Gmail via GmailDownloader
gmail_downloader = GmailDownloader()
downloaded_files = gmail_downloader.download_excel_attachments(max_emails=3)

# CR√çTICO: Processamento Real Excel ‚Üí JSON
for file_info in downloaded_files:
    excel_path = file_info["path"]
    json_path = f"mctech/ctes/consolidated_ctes_{daily_batch_id}_{file_info['email_id'][:8]}.json"
    json_created_successfully = await process_excel_to_json(excel_path, json_path, daily_batch_id)
```

**Fun√ß√£o Principal: `process_excel_to_json(excel_path, json_path, batch_id)`**
- **Leitura Excel**: `df = pd.read_excel(excel_path, engine='pyxlsb')`
- **Filtragem CTE**: `cte_df = df[df['TIPO'] == 'CTE'].copy()`
- **Agrupamento PO**: `for po_number, po_group in cte_df.groupby('PO')`
- **Estrutura JSON**: Cria estrutura consolidada com rastreamento individual de status de PO

**Estrutura de Sa√≠da**:
```python
{
  "daily_batch_id": "daily_20250814_185139",
  "new_emails_processed": 1,
  "new_json_files_created": [
    {
      "filename": "Upload_11.08.2025 - MCTECH.xlsb",
      "json_created": "mctech/ctes/consolidated_ctes_daily_20250814_19899b68.json",
      "json_exists": True,
      "status": "NEW_PENDING"
    }
  ],
  "existing_json_files_found": ["mctech/ctes/consolidated_ctes_daily_20250813_*.json"]
}
```

##### **üîç ETAPA 2: `execute_json_analysis_step`**
**Fun√ß√£o**: `async def execute_json_analysis_step(step_input: StepInput) -> StepOutput`

**Agente**: `data_extractor = create_data_extractor_agent()`
**Processamento Real**:
```python
# Combinar todos os arquivos JSON (novos + existentes)
all_json_files = init_results["existing_json_files_found"] + [
    file_info["json_created"] for file_info in init_results["new_json_files_created"]
]

# Analisar cada JSON e extrair status do PO
for json_file_path in all_json_files:
    with open(json_file_path, encoding="utf-8") as f:
        json_data = json.load(f)
    
    orders = json_data.get("orders", [])
    for order in orders:
        po_number = order.get("po_number")
        status = order.get("status", "PENDING")
        
        # Categorizar por status
        if status == "PENDING":
            analysis_results["processing_categories"]["pending_pos"].append({
                "po_number": po_number, 
                "json_file": json_file_path
            })
```

**Estrutura de Sa√≠da**:
```python
{
  "processing_categories": {
    "pending_pos": [{"po_number": "600710247", "json_file": "..."}],
    "monitoring_pos": [{"po_number": "600710248", "json_file": "..."}],
    "download_pos": [{"po_number": "600710249", "json_file": "..."}],
    "upload_pos": [{"po_number": "600710250", "json_file": "..."}],
    "completed_pos": [{"po_number": "600710251", "json_file": "..."}],
    "failed_pos": []
  },
  "analysis_summary": {
    "total_pos_found": 22,
    "pos_needing_processing": 15,
    "pos_completed": 7
  }
}
```

##### **üéØ ETAPA 3: `execute_status_based_routing_step`**
**Fun√ß√£o**: `async def execute_status_based_routing_step(step_input: StepInput) -> StepOutput`

**Agente**: `api_orchestrator = create_api_orchestrator_agent()`
**Cria√ß√£o de Filas**:
```python
routing_results = {
  "processing_queues": {
    "invoice_generation_queue": {
      "action": "invoiceGen",
      "pos": processing_categories["pending_pos"],
      "batch_processing": True,  # M√∫ltiplos POs em uma √∫nica chamada
      "priority": 1
    },
    "invoice_monitoring_queue": {
      "action": "invoiceMonitor", 
      "pos": processing_categories["monitoring_pos"],
      "batch_processing": True,
      "priority": 2
    },
    "invoice_download_queue": {
      "action": "main-download-invoice",
      "pos": processing_categories["download_pos"],
      "batch_processing": False,  # Individual processing
      "priority": 3
    },
    "invoice_upload_queue": {
      "action": "invoiceUpload",
      "pos": processing_categories["upload_pos"], 
      "batch_processing": False,
      "priority": 4
    }
  }
}
```

##### **‚öôÔ∏è STEP 4: `execute_individual_po_processing_step`**
**Function**: `async def execute_individual_po_processing_step(step_input: StepInput) -> StepOutput`

**HTTP Client**: `api_client = BrowserAPIClient()`
**Processing Logic**:
```python
# Process each queue by priority
for queue_name, queue_data in processing_queues.items():
    action = queue_data["action"]
    pos = queue_data["pos"]
    batch_processing = queue_data["batch_processing"]
    
    if batch_processing:
        # Batch mode (invoiceGen, invoiceMonitor)
        po_numbers = [po_data["po_number"] for po_data in pos]
        payload = {
            "flow_name": action,
            "parameters": {
                "orders": po_numbers,
                "headless": True
            }
        }
        api_response = await api_client.execute_api_call(action, payload)
    else:
        # Individual mode (download, upload)
        for po_data in pos:
            payload = api_client.build_invoice_download_payload(po_details)
            api_response = await api_client.execute_api_call(action, payload)
```

**Real API Payloads**:
```json
// invoiceGen (batch)
{
  "flow_name": "invoiceGen",
  "parameters": {
    "orders": ["600710247", "600710248", "600710249"],
    "headless": true
  }
}

// main-download-invoice (individual)  
{
  "flow_name": "main-download-invoice",
  "parameters": {
    "po": "600710247",
    "ctes": ["44596", "44591", "44590"],
    "total_value": 963.85,
    "startDate": "45778",
    "endDate": "45778",
    "headless": true
  }
}
```

**Status Updates**:
```python
# Status transition mapping
new_status = {
    "invoiceGen": "WAITING_MONITORING",
    "invoiceMonitor": "MONITORED", 
    "main-download-invoice": "DOWNLOADED",
    "invoiceUpload": "UPLOADED"
}.get(action)

processing_results["status_updates"][po_number] = {
    "old_status": "PENDING",
    "new_status": "WAITING_MONITORING",
    "json_file": "mctech/ctes/file.json"
}
```

##### **üèÅ STEP 5: `execute_daily_completion_step`**
**Function**: `async def execute_daily_completion_step(step_input: StepInput) -> StepOutput`

**Agent**: `file_manager = create_file_manager_agent()`
**JSON File Updates**:
```python
# Group updates by JSON file
files_updated = {}
for po_number, update_info in status_updates.items():
    json_file = update_info["json_file"]
    if json_file not in files_updated:
        files_updated[json_file] = {
            "pos_updated": [],
            "update_count": 0
        }
    
    files_updated[json_file]["pos_updated"].append({
        "po_number": po_number,
        "status_change": f"{update_info['old_status']} ‚Üí {update_info['new_status']}"
    })
```

**Final Summary**:
```python
completion_summary = {
    "daily_execution_summary": {
        "execution_date": "2025-08-14",
        "daily_batch_id": "daily_20250814_185139",
        "total_execution_time_minutes": 15,
        "overall_status": "SUCCESS"
    },
    "processing_statistics": {
        "new_emails_processed": 1,
        "existing_files_analyzed": 3,
        "total_pos_found": 45,
        "pos_processed_today": 22,
        "pos_completed_today": 8,
        "api_calls_successful": 12,
        "api_calls_failed": 0
    },
    "next_execution_scheduled": {
        "next_run": "2025-08-15T08:00:00.000000+00:00",
        "frequency": "daily",
        "estimated_pos_for_next_run": 23
    }
}
```

#### **Session State Management**
**Inter-Step Data Flow**:
```python
# Step 1 ‚Üí Step 2
set_session_state(step_input, "initialization_results", initialization_results)

# Step 2 ‚Üí Step 3  
set_session_state(step_input, "analysis_results", analysis_results)

# Step 3 ‚Üí Step 4
set_session_state(step_input, "routing_results", routing_results) 

# Step 4 ‚Üí Step 5
set_session_state(step_input, "processing_results", processing_results)

# Access previous step data
previous_output = step_input.get_step_output("daily_initialization")
init_results = json.loads(previous_output.content)
```

#### **Error Handling & Retry Logic**
**HTTP Client Retry Strategy**:
```python
for attempt in range(self.max_retries):  # Default: 3 attempts
    try:
        result = await self._execute_real_api_call(flow_name, payload)
        return result
    except aiohttp.ClientError as e:
        if attempt < self.max_retries - 1:
            wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s
            await asyncio.sleep(wait_time)
        else:
            raise ProcessamentoFaturasError(
                f"API call to {flow_name} failed after {self.max_retries} attempts",
                "HTTPClientError",
                str(e),
                "check_browser_api_server_status"
            )
```

**Status-Based Error Recovery**:
```python
# Excel processing errors
"status": "FAILED_EXTRACTION" if not json_created_successfully else "NEW_PENDING"

# API call failures by status
FAILED_GENERATION   ‚Üí Retry invoiceGen API call
FAILED_MONITORING   ‚Üí Retry invoiceMonitor API call  
FAILED_DOWNLOAD     ‚Üí Retry individual download
FAILED_UPLOAD       ‚Üí Retry individual upload
```

#### **Real JSON Consolidated Structure**
**Complete Example from Production**:
```json
{
  "batch_info": {
    "batch_id": "daily_20250814_185139",
    "source_file": "mctech/sheets/Upload_11.08.2025 - MCTECH.xlsb",
    "processing_timestamp": "2025-08-14T18:55:21.474957+00:00",
    "total_ctes": 45,
    "total_minutas_excluded": 82
  },
  "orders": [
    {
      "po_number": "600710247",
      "status": "PENDING",
      "ctes": [
        {
          "NF/CTE": "44596",
          "valor_chave": "372.27",
          "empresa_origem": "EMBRATEL",
          "cnpj_fornecedor": "9351138000100",
          "competencia": "45778"
        }
      ],
      "cte_count": 3,
      "po_total_value": 963.85,
      "start_date": "45778",
      "end_date": "45778",
      "created_at": "2025-08-14T18:55:21.475548+00:00",
      "last_updated": "2025-08-14T18:55:21.475553+00:00"
    }
  ],
  "summary": {
    "total_orders": 22,
    "total_ctes": 45,
    "total_value": 86159.77
  }
}
```

### üîç **AN√ÅLISE DETALHADA DOS STEPS**

---

#### **üåÖ STEP 1: `daily_initialization`**

**üéØ OBJETIVO DO STEP 1:**
Inicializar o ciclo di√°rio combinando **novos emails** com **trabalho pendente** de execu√ß√µes anteriores.

**üîß OPERA√á√ïES DETALHADAS:**

**üìä 1.1 - Gera√ß√£o de Batch ID:**
```python
daily_batch_id = f"daily_{datetime.now(UTC).strftime('%Y%m%d_%H%M%S')}"
# Exemplo: "daily_20250814_185139"
```

**üìß 1.2 - Processamento de Emails Novos:**
```python
gmail_downloader = GmailDownloader()
downloaded_files = gmail_downloader.download_excel_attachments(max_emails=3)
```
- **Conecta** no Gmail via OAuth2
- **Busca** emails com label "MC Tech/N√£o Processado"  
- **Filtra** anexos Excel (.xlsx, .xlsb, .xls)
- **Baixa** at√© 3 arquivos Excel por execu√ß√£o
- **Move** emails processados para "MC Tech/Processado"

**üîÑ 1.3 - Convers√£o Excel ‚Üí JSON (NOVA IMPLEMENTA√á√ÉO):**
```python
for file_info in downloaded_files:
    excel_path = file_info["path"]
    json_path = f"mctech/ctes/consolidated_ctes_{daily_batch_id}_{file_info['email_id'][:8]}.json"
    json_created_successfully = await process_excel_to_json(excel_path, json_path, daily_batch_id)
```

**Detalhes da convers√£o:**
- **L√™ Excel** com pandas + pyxlsb engine
- **Filtra CTEs**: `df[df['TIPO'] == 'CTE']` (exclui MINUTAS)  
- **Agrupa por PO**: Purchase Orders individuais
- **Cria estrutura JSON**:
  ```json
  {
    "batch_info": { "batch_id": "...", "total_ctes": 45 },
    "orders": [
      {
        "po_number": "600710247",
        "status": "PENDING",
        "ctes": [...],
        "po_total_value": 963.85
      }
    ]
  }
  ```

**üìÅ 1.4 - Scan de JSONs Existentes:**
```python
json_pattern = "mctech/ctes/consolidated_ctes_*.json"
existing_json_files = glob.glob(json_pattern)
```
- **Encontra** todos os JSONs de execu√ß√µes anteriores
- **Lista** caminhos completos dos arquivos

**üì§ 1.5 - Output do Step:**
```python
{
  "daily_batch_id": "daily_20250814_185139",
  "new_emails_processed": 1,
  "new_json_files_created": [...],  # JSONs criados hoje
  "existing_json_files_found": [...], # JSONs de dias anteriores  
  "total_files_to_analyze": 5
}
```

---

#### **üîç STEP 2: `json_analysis`**

**üéØ OBJETIVO DO STEP 2:**
Analisar **TODOS** os JSONs (novos + existentes) e extrair o status individual de cada PO para determinar pr√≥ximas a√ß√µes necess√°rias.

**üîß OPERA√á√ïES DETALHADAS:**

**ü§ñ 2.1 - Agent de An√°lise:**
```python
data_extractor = create_data_extractor_agent()
response = data_extractor.run(analysis_context)
```
- **Agent** especializado em an√°lise de dados estruturados
- **Contextualiza** a an√°lise com instru√ß√µes espec√≠ficas

**üìã 2.2 - Coleta de Arquivos:**
```python
all_json_files = init_results["existing_json_files_found"] + [
    file_info["json_created"] for file_info in init_results["new_json_files_created"]
]
```
- **Combina** JSONs existentes + JSONs rec√©m-criados
- **Lista √∫nica** de todos os arquivos para an√°lise

**üîç 2.3 - An√°lise de Cada JSON:**
```python
for json_file_path in all_json_files:
    with open(json_file_path, encoding="utf-8") as f:
        json_data = json.load(f)
    
    orders = json_data.get("orders", [])
    for order in orders:
        po_number = order.get("po_number")
        status = order.get("status", "PENDING")
```

**üìä 2.4 - Categoriza√ß√£o por Status:**
Cada PO √© categorizado conforme seu status atual:
- **`PENDING`** ‚Üí `pending_pos[]` ‚Üí Pronto para invoiceGen API  
- **`WAITING_MONITORING`** ‚Üí `monitoring_pos[]` ‚Üí Pronto para invoiceMonitor API
- **`MONITORED`** ‚Üí `download_pos[]` ‚Üí Pronto para download API
- **`DOWNLOADED`** ‚Üí `upload_pos[]` ‚Üí Pronto para upload API
- **`UPLOADED`** ‚Üí `completed_pos[]` ‚Üí Completado (skip)
- **`FAILED_*`** ‚Üí `failed_pos[]` ‚Üí Precisa retry/corre√ß√£o

**üì§ 2.5 - Output do Step:**
```python
{
  "processing_categories": {
    "pending_pos": [{"po_number": "600710247", "json_file": "..."}],
    "monitoring_pos": [...],
    "download_pos": [...],
    "upload_pos": [...],
    "completed_pos": [...],
    "failed_pos": [...]
  },
  "analysis_summary": {
    "total_pos_found": 22,
    "pos_needing_processing": 15,
    "pos_completed": 7
  }
}
```

---

#### **üéØ STEP 3: `status_based_routing`**

**üéØ OBJETIVO DO STEP 3:**
Organizar POs em **filas de processamento** otimizadas por tipo de opera√ß√£o e prioridade, preparando a execu√ß√£o eficiente das APIs.

**üîß OPERA√á√ïES DETALHADAS:**

**ü§ñ 3.1 - Agent de Orquestra√ß√£o:**
```python
api_orchestrator = create_api_orchestrator_agent()
response = api_orchestrator.run(routing_context)
```
- **Agent** especializado em coordena√ß√£o de APIs
- **Planeja** sequ√™ncia otimizada de execu√ß√£o

**üìã 3.2 - Cria√ß√£o de Filas Priorizadas:**

**ü•á FILA 1 - invoice_generation_queue (Prioridade 1):**
```python
{
  "action": "invoiceGen",
  "pos": processing_categories["pending_pos"],
  "batch_processing": True,  # M√∫ltiplos POs em 1 call
  "priority": 1
}
```
- **Input**: POs com status `PENDING`
- **A√ß√£o**: Gerar faturas via Browser API
- **Output**: Status ‚Üí `WAITING_MONITORING`

**ü•à FILA 2 - invoice_monitoring_queue (Prioridade 2):**
```python
{
  "action": "invoiceMonitor", 
  "pos": processing_categories["monitoring_pos"],
  "batch_processing": True,  # M√∫ltiplos POs em 1 call
  "priority": 2
}
```
- **Input**: POs com status `WAITING_MONITORING`
- **A√ß√£o**: Monitorar conclus√£o da gera√ß√£o
- **Output**: Status ‚Üí `MONITORED`

**ü•â FILA 3 - invoice_download_queue (Prioridade 3):**
```python
{
  "action": "main-download-invoice",
  "pos": processing_categories["download_pos"], 
  "batch_processing": False,  # Individual por PO
  "priority": 3
}
```
- **Input**: POs com status `MONITORED`
- **A√ß√£o**: Download individual de cada fatura
- **Output**: Status ‚Üí `DOWNLOADED`

**üèÖ FILA 4 - invoice_upload_queue (Prioridade 4):**
```python
{
  "action": "invoiceUpload",
  "pos": processing_categories["upload_pos"],
  "batch_processing": False,  # Individual por PO  
  "priority": 4
}
```
- **Input**: POs com status `DOWNLOADED`
- **A√ß√£o**: Upload individual de cada fatura
- **Output**: Status ‚Üí `UPLOADED`

**üìä 3.3 - Plano de Execu√ß√£o:**
```python
"execution_plan": {
  "total_actions": 22,
  "batch_actions": 2,        # invoiceGen + invoiceMonitor
  "individual_actions": 15,  # downloads + uploads individuais
  "estimated_execution_time_minutes": 15
}
```

**üì§ 3.4 - Output do Step:**
Filas organizadas e prontas para execu√ß√£o sequencial por prioridade.

---

#### **‚öôÔ∏è STEP 4: `individual_po_processing`**

**üéØ OBJETIVO DO STEP 4:**
Executar as **APIs do Browser** em sequ√™ncia otimizada, processando cada fila conforme sua prioridade e modo (batch vs individual).

**üîß OPERA√á√ïES DETALHADAS:**

**üåê 4.1 - Cliente HTTP:**
```python
api_client = BrowserAPIClient()
# URL: http://localhost:8088 (configur√°vel)
# Timeout: 900s, Max retries: 3
```

**üîÑ 4.2 - Processamento por Prioridade:**
```python
for queue_name, queue_data in processing_queues.items():
    action = queue_data["action"]
    pos = queue_data["pos"]  
    batch_processing = queue_data["batch_processing"]
```

**üì¶ 4.3 - Modo BATCH (Filas 1-2):**
```python
if batch_processing:  # invoiceGen, invoiceMonitor
    po_numbers = [po_data["po_number"] for po_data in pos]
    payload = {
        "flow_name": action,
        "parameters": {
            "orders": po_numbers,  # ["600710247", "600710248", ...]
            "headless": True
        }
    }
    api_response = await api_client.execute_api_call(action, payload)
```

**Exemplo payload invoiceGen:**
```json
{
  "flow_name": "invoiceGen",
  "parameters": {
    "orders": ["600710247", "600710248", "600710249"],
    "headless": true
  }
}
```

**‚ö° Atualiza√ß√£o de Status em Batch:**
```python
if api_response["success"]:
    new_status = {
        "invoiceGen": "WAITING_MONITORING", 
        "invoiceMonitor": "MONITORED"
    }.get(action)
    
    for po_data in pos:
        status_updates[po_number] = {
            "old_status": "PENDING",
            "new_status": "WAITING_MONITORING",
            "json_file": "mctech/ctes/file.json"
        }
```

**üîÑ 4.4 - Modo INDIVIDUAL (Filas 3-4):**
```python
else:  # main-download-invoice, invoiceUpload
    for po_data in pos:
        po_details = load_po_from_json(po_data["json_file"])
        
        if action == "main-download-invoice":
            payload = api_client.build_invoice_download_payload(po_details)
        elif action == "invoiceUpload":  
            payload = api_client.build_invoice_upload_payload(po_details, file_path)
            
        api_response = await api_client.execute_api_call(action, payload)
```

**Exemplo payload download:**
```json
{
  "flow_name": "main-download-invoice",
  "parameters": {
    "po": "600710247",
    "ctes": ["44596", "44591", "44590"],
    "total_value": 963.85,
    "startDate": "45778",
    "endDate": "45778", 
    "headless": true
  }
}
```

**üîÑ 4.5 - Sistema de Retry com Exponential Backoff:**
```python
for attempt in range(self.max_retries):  # 3 tentativas
    try:
        result = await self._execute_real_api_call(flow_name, payload)
        return result
    except Exception as e:
        if attempt < self.max_retries - 1:
            wait_time = 2 ** attempt  # 1s, 2s, 4s
            await asyncio.sleep(wait_time)
```

**üìä 4.6 - Tracking de Execu√ß√µes:**
```python
processing_results = {
    "api_executions": {
        "invoice_generation_queue": {
            "action": "invoiceGen",
            "pos_processed": ["600710247", "600710248"],
            "batch_mode": True,
            "success": True,
            "execution_time_ms": 15000
        }
    },
    "status_updates": {
        "600710247": {
            "old_status": "PENDING",
            "new_status": "WAITING_MONITORING",
            "json_file": "mctech/ctes/file.json"
        }
    },
    "execution_summary": {
        "successful_actions": 4,
        "failed_actions": 0,
        "pos_updated": 22
    }
}
```

---

#### **üèÅ STEP 5: `daily_completion`**

**üéØ OBJETIVO DO STEP 5:**
Finalizar o ciclo di√°rio atualizando os JSONs com novos status, gerando relat√≥rio completo e agendando pr√≥xima execu√ß√£o.

**üîß OPERA√á√ïES DETALHADAS:**

**ü§ñ 5.1 - Agent de Gerenciamento:**
```python
file_manager = create_file_manager_agent()
response = file_manager.run(completion_context)
```
- **Agent** especializado em opera√ß√µes de arquivo
- **Coordena** atualiza√ß√µes de estado

**üíæ 5.2 - Atualiza√ß√£o de JSONs:**
```python
files_updated = {}
for po_number, update_info in status_updates.items():
    json_file = update_info["json_file"]
    files_updated[json_file]["pos_updated"].append({
        "po_number": po_number,
        "status_change": f"PENDING ‚Üí WAITING_MONITORING"
    })
```
- **Agrupa** atualiza√ß√µes por arquivo JSON
- **Preserva** estrutura original dos JSONs
- **Atualiza** apenas campos de status e timestamp

**üìä 5.3 - Consolida√ß√£o de Estat√≠sticas:**
```python
completion_summary = {
    "daily_execution_summary": {
        "execution_date": "2025-08-14",
        "daily_batch_id": "daily_20250814_185139", 
        "total_execution_time_minutes": 15,
        "overall_status": "SUCCESS"
    },
    "processing_statistics": {
        "new_emails_processed": 1,
        "existing_files_analyzed": 3,
        "total_pos_found": 45,
        "pos_processed_today": 22,
        "pos_completed_today": 8,  # Reached UPLOADED
        "api_calls_successful": 12,
        "api_calls_failed": 0
    }
}
```

**‚è∞ 5.4 - Agendamento da Pr√≥xima Execu√ß√£o:**
```python
"next_execution_scheduled": {
    "next_run": "2025-08-15T08:00:00.000000+00:00",
    "frequency": "daily",
    "estimated_pos_for_next_run": 23  # POs ainda pendentes
}
```

**üìã 5.5 - Rastreamento de Transi√ß√µes:**
```python
"status_transitions_applied": {
    "600710247": {
        "old_status": "PENDING",
        "new_status": "WAITING_MONITORING", 
        "json_file": "mctech/ctes/consolidated_ctes_daily_20250814_19899b68.json"
    },
    "600710248": {
        "old_status": "WAITING_MONITORING",
        "new_status": "MONITORED",
        "json_file": "mctech/ctes/consolidated_ctes_daily_20250813_ab123cd4.json" 
    }
}
```

### üéØ **RESUMO EXECUTIVO DO WORKFLOW**

Este workflow √© uma **obra-prima de automa√ß√£o robusta e inteligente!** üöÄ Combina processamento em tempo real, persist√™ncia de estado, orquestra√ß√£o de APIs e recupera√ß√£o autom√°tica em um sistema altamente eficiente para processamento de CTEs.


Co-Authored-By: Automagik Genie <genie@namastex.ai>
